---
title: "4 - Evaluating models - Classwork"
subtitle: "Introduction to tidymodels"
editor_options: 
  chunk_output_type: console
---

We recommend restarting R between each slide deck!

## Setup

Setup from deck 3

```{r}
library(tidymodels)
library(forested)

set.seed(123)
forested_split <- initial_split(forested, prop = 0.8)
forested_train <- training(forested_split)
forested_test <- testing(forested_split)

# decrease cost_complexity from its default 0.01 to make a more
# complex and performant tree. see `?decision_tree()` to learn more.
tree_spec <- decision_tree(cost_complexity = 0.0001, mode = "classification")
forested_wflow <- workflow(forested ~ ., tree_spec)
forested_fit <- fit(forested_wflow, forested_train)
```

## Metrics for model performance

`conf_mat()` can be used to see how well the model is doing at prediction

```{r}
augment(forested_fit, new_data = forested_train) %>%
  conf_mat(truth = forested, estimate = .pred_class)
```

and it has nice plotting features

```{r}
augment(forested_fit, new_data = forested_train) %>%
  conf_mat(truth = forested, estimate = .pred_class) %>%
  autoplot(type = "heatmap")
```

using the same interface we can calculate metrics

```{r}
augment(forested_fit, new_data = forested_train) %>%
  accuracy(truth = forested, estimate = .pred_class)

augment(forested_fit, new_data = forested_train) %>%
  specificity(truth = forested, estimate = .pred_class)

augment(forested_fit, new_data = forested_train) %>%
  sensitivity(truth = forested, estimate = .pred_class)
```

Metric sets are a way to combine multiple similar metric functions together into a new function.

```{r}
forested_metrics <- metric_set(accuracy, specificity, sensitivity)

augment(forested_fit, new_data = forested_train) %>%
  forested_metrics(truth = forested, estimate = .pred_class)
```

Metrics and metric sets work with grouped data frames!

```{r}
augment(forested_fit, new_data = forested_train) %>%
  group_by(tree_no_tree) %>%
  accuracy(truth = forested, estimate = .pred_class)
```

## Your turn

Apply the `forested_metrics` metric set to `augment()` output grouped by `tree_no_tree`.

Do any metrics differ substantially between groups?

```{r}
# Your code here!
augment(forested_fit, new_data = forested_train) %>%
  group_by(tree_no_tree) %>%
  forested_metrics(truth = forested, estimate = .pred_class)
```
*reminder to use the training dataset*
-  specificity of `Tree` is relatively low (`unforested` but is `Tree`) & the sensitivity of `No tree` is lower than the rest of the metrics (except `Tree` specificity)

## Your turn

Compute and plot an ROC curve for your current model.

What data are being used for this ROC curve plot?

```{r}
# Your code here!
# Assumes _first_ factor level is event; there are options to change that
augment(forested_fit, new_data = forested_train) %>% 
  roc_curve(truth = forested, .pred_Yes) %>%
  slice(1, 20, 50)
#> # A tibble: 3 × 3
#>   .threshold specificity sensitivity
#>        <dbl>       <dbl>       <dbl>
#> 1   -Inf           0           1    
#> 2      0.235       0.885       0.972
#> 3      0.909       0.969       0.826

augment(forested_fit, new_data = forested_train) %>% 
  roc_auc(truth = forested, .pred_Yes)
#> # A tibble: 1 × 3
#>   .metric .estimator .estimate
#>   <chr>   <chr>          <dbl>
#> 1 roc_auc binary         0.975

augment(forested_fit, new_data = forested_train) %>% 
  roc_curve(truth = forested, .pred_Yes) %>%
  autoplot()
```

## Dangers of overfitting

Repredicting the training set, bad!

```{r}
forested_fit %>%
  augment(forested_train)
```

"Resubstitution estimate" - This should be the best possible performance that you could ever achieve, but it can be very misleading!

```{r}
forested_fit %>%
  augment(forested_train) %>%
  accuracy(forested, .pred_class)
```

Now on the test set, see that it performs worse? This is closer to "real" performance.

```{r}
forested_fit %>%
  augment(forested_test) %>%
  accuracy(forested, .pred_class)
```

## Your turn

Use `augment()` and and a metric function to compute a classification metric like `brier_class()`.

Compute the metrics for both training and testing data to demonstrate overfitting!

Notice the evidence of overfitting!

```{r}
# Your code here!

# Use `augment()` and `brier_class()` with `forested_fit`
augment(forested_fit, new_data = forested_train) %>% 
  brier_class(truth = forested, .pred_Yes)

augment(forested_fit, new_data = forested_test) %>% 
  brier_class(truth = forested, .pred_Yes)
```
Smaller values are better, for binary classification the *“bad model threshold”* is about 0.25.

-  model is not performing so well, but we shouldn't have used the testing set to determine this (i.e. without *resubstitution*)
-  if we test a bunch of different models, how can we tell if the differences are important?

## Your turn

If we use 10 folds, what percent of the training data:

- ends up in analysis?
- ends up in assessment?

for each fold

## Resampling

```{r}
# v = 10 is the default
vfold_cv(forested_train)
```
This is different for a Monte Carlo cross-validation, where you can further control the number of random rows used to assess. Here, it is controlled by the number of folds.

What is in a resampling result?

```{r}
forested_folds <- vfold_cv(forested_train, v = 10)

# Individual splits of analysis/assessment data
forested_folds$splits[1:3]
```

We'll use this setup:

```{r}
set.seed(123)
forested_folds <- vfold_cv(forested_train, v = 10)
forested_folds
```
Set the seed when creating resamples for reproducibility.

## Evaluating model performance

```{r}
# Fit the workflow on each analysis set,
# then compute performance on each assessment set
forested_res <- fit_resamples(forested_wflow, forested_folds)
forested_res
```

Aggregate metrics

```{r}
forested_res %>%
  collect_metrics()
```
This resulting table shows the mean and sd values for each metric across our 10 folds.
This shows that we can reliably measure performance using only the training data!

If you want to analyze the assessment set (i.e. holdout) predictions, then you need to adjust the control object and tell it to save them:

```{r}
# Save the assessment set results
ctrl_forested <- control_resamples(save_pred = TRUE)

forested_res <- fit_resamples(forested_wflow, forested_folds, control = ctrl_forested)

forested_preds <- collect_predictions(forested_res)
forested_preds
```

## Bootstrapping

```{r}
set.seed(3214)
bootstraps(forested_train)
```

## Your turn

Create:

- Monte Carlo Cross-Validation sets
- validation set

(use the reference guide to find the functions)

https://rsample.tidymodels.org/reference/index.html

Don't forget to set a seed when you resample!

```{r}
# Your code here!
set.seed(322)
mc_cv(forested_train, times = 10)

set.seed(853)
forested_val_split <- 
  # generates the validation object
  initial_validation_split(forested)
validation_set(forested_val_split)
```
MC cross-validation takes a random sample (without replacement) of the original data set to be used for analysis. All other data points are added to the assessment set.

A validation set is just another type of resample.

## Create a random forest model

```{r}
rf_spec <- rand_forest(trees = 1000, mode = "classification")
rf_spec
```

```{r}
rf_wflow <- workflow(forested ~ ., rf_spec)
rf_wflow
```

## Your turn

Use `fit_resamples()` and `rf_wflow` to:

- Keep predictions
- Compute metrics

```{r}
# Your code here!
ctrl_forested <- control_resamples(save_pred = TRUE)

# Random forest uses random numbers so set the seed first

set.seed(2)
rf_res <- fit_resamples(rf_wflow, forested_folds, control = ctrl_forested)
collect_metrics(rf_res)
```


## The final fit

```{r}
# `forested_split` has train + test info
final_fit <- last_fit(rf_wflow, forested_split) 

final_fit
```

Test set metrics:

```{r}
collect_metrics(final_fit)
```

Test set predictions:

```{r}
collect_predictions(final_fit)
```

```{r}
collect_predictions(final_fit) %>%
  ggplot(aes(.pred_class, fill = forested)) + 
  geom_bar() 
```

```{r}
extract_workflow(final_fit)
```
